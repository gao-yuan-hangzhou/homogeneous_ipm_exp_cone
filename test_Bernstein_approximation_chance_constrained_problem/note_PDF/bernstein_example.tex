\documentclass[11pt]{article}
\usepackage{graphicx,amsmath,amsthm, amssymb,setspace}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 8.5in}]{geometry}
\usepackage{cite}

%opening
\title{Bernstein approximation of chance constrained problems: an example}
\author{Gao Yuan}

\begin{document}

\maketitle

\begin{abstract}
We study the example in\cite{Nemirovsky_and_Shapiro} in more detail and repeat the computation using the new conic program solver.
\end{abstract}

\section*{The model}
We describe the chance constrained problem in detail. As in \cite{Nemirovsky_and_Shapiro}, consider the following chance constrained program
\begin{align} \label{investment_problem}
\max_{x_0, x_1\cdots,x_n, \tau}\ (\tau - 1)\ \ \ \textnormal{s.t.}\ \ \mathbb{P} \left(\tau > \sum_{j=0}^n r_j x_j \right) \leq \alpha,\ \sum_{j=0}^{n} x_j \leq 1,\ x_j \geq 0, \forall j
\end{align}
where $\alpha \in [0,1]$ is a given constant. The assumptions are 
\begin{enumerate}
	\item The returns $r_0, r_1, \cdots, r_n$ satisfy $r_0=1$ and $\mathbb{E}(r_i) = 1 + \rho_i$ with $0\leq \rho_1 \leq \cdots \leq \rho_n$.
	\item For $1\leq j \leq n$ and $1\leq l \leq q$, one has $r_j = \eta_j + \sum_{l=1}^q \gamma_{jl}\zeta_l$ where $\eta_j \sim \mathcal{LN}(\mu_j, \sigma_j^2)$ (the individual noises) and $\zeta_l \sim \mathcal{LN}(\nu_l, \theta_l^2)$. All $\eta_j$ and $\zeta_l$ are independent of each other.
	\item One has $\nu_l = 0$, $\theta_l=0.1$ for all $l$, $\mu_j = \sigma_j$ for all $j$, $\sum_{l=1}^q \gamma_{jl} \exp \left(\nu_l + \dfrac{\theta_l^2}{2}\right) = \dfrac{\rho_j}{2}$ for all $j$ and $\sum_{j=1}^n \exp\left(\mu_j + \dfrac{\sigma_j^2}{2}\right) = 1 + \dfrac{\rho_j}{2}$.
\end{enumerate}
We see that the problem can be rewritten into (1.1) in \cite{Nemirovsky_and_Shapiro} with $m=1$. Denote \[\tilde{x} = (\tau, x_0, x_1, \cdots, x_n)^T.\] The objective function is simply $f(\tilde{x}) = -\tau$, and the chance constraint is \[\mathbb{P}\left(F(\tilde{x}, \xi) \leq 0 \right) \geq 1-\alpha\] where \[F(\tilde{x},\xi) =g_0(\tilde{x}) + \sum_{j=1}^d \xi_j g_j(\tilde{x}),\ d = n+q,\ g_0(\tilde{x}) = \tau - x_0,\]
\[\xi_j = \eta_j,\ g_j(\tilde{x}) = -x_j,\ 1\leq j \leq n,\] \[\xi_{n+l} = \zeta_l,\ g_{n+l}(\tilde{x}) = -\sum_{j=1}^n \gamma_{jl}x_j,\ 1\leq l \leq q.\]

\section*{Convex approximation and standard form formulation }
Here we construct the Bernstein approximation to \eqref{investment_problem} and reformulate it into a standard form involving exponential cone constraints. \\

Note that the discretization scheme described in \cite{Nemirovsky_and_Shapiro} has been adopted and all random variables $\xi_j$, $1\leq j \leq d$ are now discrete with finite support. For each $j$, denote the support and the associated probability masses as $\left\{(v_k^j, p_k^j)\right\}_{1\leq k \leq N_j}$. In other words, for each $j$, $\mathbb{P}\left(\xi_j = v_k^j\right), \forall k$ and the moment generating function of $\xi_j$ is $M_j: z \rightarrow \sum_{k=1}^{N_j} p_k^j \exp \left(v_k^j z\right)$.\\

The Bernstein approximation to (1) is therefore the following convex maximization problem
\begin{align} \label{Bernstein_approx_direct_form}
\max_{x_0, x_1, \cdots, x_n, \tau} (\tau - 1)\ \ \ \ \textnormal{s.t.}\ \ \inf_{t>0} \left(g_0(\tilde{x}) + \sum_{j=1}^d t \Lambda_j\left(t^{-1}g_j(\tilde{x})\right) - t\log \alpha \right) \leq 0.
\end{align}

In fact, problem \eqref{Bernstein_approx_direct_form} can be reformulated into the standard form (PD$'$) in \cite{Gao_Yuan_thesis}, namely (note that $d = n+q$)
\begin{align}
\begin{split}\label{reformulated_std_conic_form}
&\min \ -\tau \\ 
\textnormal{s.t.}\ \ \ & x_0 + x_1 + \cdots + x_n + s_x = 1 \\
& g_0 + \left(\sum_{j=1}^d s_j\right) - \left(\log \alpha\right) t_0 = 0 \\
& g_0  - \tau + x_0 = 0 \\
& g_j + x_j = 0,\ j = 1, \cdots, n \\
& g_{n+l} + \sum_{j=1}^n \gamma_{jl} x_j = 0,\ l = 1, \cdots, q \\
& w_k^j - v_k^j g_j + s_j = 0,\ j = 1, \cdots,d,\ k = 1, \cdots, N_j \\
& \sum_{k=1}^{N_j} p_k^j u_k^j - t_0 = 0,\  j =1,\cdots,d \\
& t_0 - t_k^j = 0,\ j = 1,\cdots, d,\ k = 1,\cdots,N_j
\end{split}
\end{align}
where the decision variables are
\begin{align*}
& \tau \in \mathbb{R} \\
& x_0, x_1, \cdots, x_n, s_x \geq 0 \\
& g_0, g_1, \cdots, g_d \in \mathbb{R} \\
& t_0 \geq 0 \\
& s_1, \cdots, s_d \in \mathbb{R} \\
& [w_k^j; u_k^j; t_k^j] \in \mathcal{K}_{\exp},\ j = 1, \cdots, d,\ k = 1,\cdots, N_j.
\end{align*}

Note that we keep the slack variable $s_x \geq 0$ in the first constraint, although it can be shown that there is always an optimal solution $(x_0^*, x_1^*, \cdots, x_n^*)$ with $\sum_{j=0}^n x_j^* = 1$.

\bibliography{references}{}
\bibliographystyle{plain}
\end{document}
