\documentclass[11pt]{article}
\usepackage{graphicx,amsmath,amsthm, amssymb,setspace}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 8.5in}]{geometry}
\usepackage{cite}

%opening
\title{Bernstein approximation of chance constrained problems: an example}
\author{Yuan Gao}

\begin{document}

\maketitle

\begin{abstract}
We study the example in\cite{Nemirovsky_and_Shapiro} in more detail and repeat the computation using the new conic program solver.
\end{abstract}

\section*{The model}
We describe the chance constrained problem in detail. As in \cite{Nemirovsky_and_Shapiro}, consider the following chance constrained program
\begin{align} \label{investment_problem}
\max_{\begin{matrix}
	\tau \in \mathbb{R}\\ 	
	x_0, x_1\cdots,x_n \geq 0
	\end{matrix}}\ (\tau - 1)\ \ \ \textnormal{s.t.}\ \ \mathbb{P} \left(\tau > \sum_{j=0}^n r_j x_j \right) \leq \alpha,\ \sum_{j=0}^{n} x_j \leq 1
\end{align}
where $\alpha \in [0,1]$ is a given constant. The assumptions are 
\begin{enumerate}
	\item The returns $r_0, r_1, \cdots, r_n$ satisfy $r_0=1$ and $\mathbb{E}(r_i) = 1 + \rho_i$ with $0\leq \rho_1 \leq \cdots \leq \rho_n$.
	\item For $1\leq j \leq n$ and $1\leq l \leq q$, one has $r_j = \eta_j + \sum_{l=1}^q \gamma_{jl}\zeta_l$ where $\eta_j \sim \mathcal{LN}(\mu_j, \sigma_j^2)$ (the individual noises) and $\zeta_l \sim \mathcal{LN}(\nu_l, \theta_l^2)$. All $\eta_j$ and $\zeta_l$ are independent of each other.
	\item One has $\nu_l = 0$, $\theta_l=0.1$, $l=1,\cdots, q$,
    $\mu_j = \sigma_j$, $j=1\cdots, n$ and
    \begin{align*}
    &\mathbb{E}\left[\sum_{l=1}^q \gamma_{jl} \zeta_l\right] =  \sum_{l=1}^q \gamma_{jl} \exp \left(\nu_l + \dfrac{\theta_l^2}{2}\right) = \dfrac{\rho_j}{2},\ j = 1, \cdots, n\\
    &\mathbb{E}\left[\eta_j\right] = \exp\left(\mu_j + \dfrac{\sigma_j^2}{2}\right) = 1 + \dfrac{\rho_j}{2},\ j =1,\cdots, n
    \end{align*}
\end{enumerate}
We see that the problem can be rewritten into (1.1) in \cite{Nemirovsky_and_Shapiro} with $m=1$. Denote \[\tilde{x} = (\tau, x_0, x_1, \cdots, x_n)^T.\] The objective function is simply $f(\tilde{x}) = -\tau$, and the chance constraint is \[\mathbb{P}\left(F(\tilde{x}, \xi) \leq 0 \right) \geq 1-\alpha\] where \[F(\tilde{x},\xi) =g_0(\tilde{x}) + \sum_{j=1}^d \xi_j g_j(\tilde{x}),\ d = n+q,\ g_0(\tilde{x}) = \tau - x_0,\]
\[\xi_j = \eta_j,\ g_j(\tilde{x}) = -x_j,\ 1\leq j \leq n,\] \[\xi_{n+l} = \zeta_l,\ g_{n+l}(\tilde{x}) = -\sum_{j=1}^n \gamma_{jl}x_j,\ 1\leq l \leq q.\]

\section*{The Bernstein approximation and standard form formulation}
Here we construct the Bernstein approximation to \eqref{investment_problem}, which is a convex optimization problem and reformulate it into a standard form conic program involving exponential cone constraints. \\

Note that the discretization scheme described in \cite{Nemirovsky_and_Shapiro} has been adopted and all random variables $\xi_j$, $1\leq j \leq d$ are now discrete with finite support. For each $j$, denote the support and the associated probability masses as $\left\{(v_k^j, p_k^j)\mid k = 1,\cdots, N_j \right\}$. In other words, for each $j$, $k=1, \cdots, N_j$, one has $\xi_j \in \left\{v_k^j \mid k=1,\cdots, N_j \right\}$ and $\mathbb{P}\left(\xi_j = v_k^j\right) = p_k^j$ and the moment generating function of $\xi_j$ is $M_j: z \rightarrow \sum_{k=1}^{N_j} p_k^j \exp \left(v_k^j z\right)$.\\

The Bernstein approximation to (1) is therefore the following convex maximization problem
\begin{align} \label{Bernstein_approx_direct_form}
\begin{split}
&\max_{\begin{matrix}
	\tau \in \mathbb{R}\\ 	
	x_0, x_1\cdots,x_n \geq 0
	\end{matrix}} (\tau - 1)\ \ \
\textnormal{s.t.}\ \ \sum_{j=0}^n x_j \leq 1,\ \ \inf_{t>0} \left(g_0(\tilde{x}) + \sum_{j=1}^d t \Lambda_j\left(t^{-1}g_j(\tilde{x})\right) - t\log \alpha \right) \leq 0
\end{split}
\end{align}

Note that problem \eqref{Bernstein_approx_direct_form} is equivalent to
\begin{align}\label{Bernstein_approx_eq_form_1}
\begin{split}
&\max_{\begin{matrix}
	\tau \in \mathbb{R} \\ 
	x_0, x_1, \cdots, x_n \geq 0\\
	g_0,\ g_1, \cdots, g_d \in \mathbb{R}\\
	s_1, \cdots, s_d \in \mathbb{R}
	\end{matrix}} (\tau - 1)\\ \textnormal{s.t.}\ \ & \sum_{j=0}^n x_j \leq 1\\
&g_0 + \sum_{j=1}^d s_j - t\log \alpha = 0 \\
{(*)_j}:\ \ \ & s_j \geq t \Lambda_j\left(\dfrac{g_j}{t}\right),\ j=1, \cdots, d\ \ \ \\
& g_0 = \tau - x_0 \\
& g_j = -x_j,\ j = 1, \cdots, n \\
& g_{n+l} = -\sum_{j=1}^n \gamma_{jl}x_j,\ l=1,\cdots, q
\end{split}
\end{align}

Since $\Lambda_j\left(\cdot\right) = \log M_j \left(\cdot\right)$, for $j=1, \cdots, d$, constraint $(*)_j$ in \eqref{Bernstein_approx_eq_form_1} is equivalent to 
\begin{align*}
&\sum_{k=1}^{N_j} p_k^j \exp \left(v_k^j \cdot \dfrac{g_j}{t}\right) \leq \exp\left(\dfrac{s_j}{t}\right) \\
\Leftrightarrow\ & \sum_{k=1}^{N_j} p_k^j \exp\left(\dfrac{v_k^j g_j - s_j}{t}\right) \leq 1 \\
\Leftrightarrow\ & \sum_{k=1}^{N_j} p_k^j\cdot  t \exp\left(\dfrac{v_k^j g_j - s_j}{t}\right) \leq t \\
\Leftrightarrow\ & \sum_{k=1}^{N_j} p_k^ju_k^j = t,\ \ \ t\exp\left(\dfrac{w_k^j}{t}\right) \leq u_k^j,\ w_k^j =v_k^j g_j - s_j,\ k = 1, \cdots, N_j \\
\Leftrightarrow\ & \sum_{k=1}^{N_j} p_k^ju_k^j = t,\ \ \ \left[w_k^j; u_k^j; t\right] \in \mathcal{K}_{\exp},\ w_k^j =v_k^j g_j - s_j,\ k = 1, \cdots, N_j.
\end{align*}

Eventually, problem \eqref{Bernstein_approx_direct_form} can be reformulated into the standard form (PD$'$) in \cite{Gao_Yuan_thesis}, namely (note that $d = n+q$ and the constant term in the objective has been dropped)
\begin{align}
\begin{split}\label{reformulated_std_conic_form}
&\min \ -\tau \\ 
\textnormal{s.t.}\ \ \ & x_0 + x_1 + \cdots + x_n + s_x = 1 \\
& g_0 + \left(\sum_{j=1}^d s_j\right) - \left(\log \alpha\right) t_0 = 0 \\
& g_0  - \tau + x_0 = 0 \\
& g_j + x_j = 0,\ j = 1, \cdots, n \\
& g_{n+l} + \sum_{j=1}^n \gamma_{jl} x_j = 0,\ l = 1, \cdots, q \\
& w_k^j - v_k^j g_j + s_j = 0,\ j = 1, \cdots,d,\ k = 1, \cdots, N_j \\
& \sum_{k=1}^{N_j} p_k^j u_k^j - t_0 = 0,\  j =1,\cdots,d \\
& t_0 - t_k^j = 0,\ j = 1,\cdots, d,\ k = 1,\cdots,N_j
\end{split}
\end{align}
with decision variables
\begin{align*}
& \tau \in \mathbb{R} \\
& x_0, x_1, \cdots, x_n, s_x \geq 0 \\
& g_0, g_1, \cdots, g_d \in \mathbb{R} \\
& t_0 \geq 0 \\
& s_1, \cdots, s_d \in \mathbb{R} \\
& \left[w_k^j; u_k^j; t_k^j\right] \in \mathcal{K}_{\exp},\ j = 1, \cdots, d,\ k = 1,\cdots, N_j.
\end{align*}

Note that we keep the slack variable $s_x \geq 0$ in the first constraint, although it can be shown that there is always an optimal solution $\left(x_0^*, x_1^*, \cdots, x_n^*\right)$ with $\sum_{j=0}^n x_j^* = 1$.

\section*{The nominal problem}
We define the \textit{nominal problem} as the problem with all random variables replaced by constants equal to their respective mean values. Specifically, the nominal problem associated with \eqref{investment_problem} is
\begin{align}\label{nominal_problem}
& \max_{\begin{matrix}\
	\tau \in \mathbb{R} \\
	x_0, x_1, \cdots, x_n \geq 0
	\end{matrix}}\ \left(\tau - 1\right)\ \ \ \ \textnormal{s.t.}\ \ \tau \leq \sum_{j=0}^n \left(1+\rho_j\right) x_j,\  \sum_{j=0}^n x_j \leq 1.
\end{align}
Since $\rho_1 \leq \cdots \leq \rho_n$,  it can be easily seen that \eqref{nominal_problem} has an optimal objective $(1+\rho_n)$ with optimal solution $x_0 = x_1 = \cdots = x_{n-1}=0,\ x_n=1$.

\bibliography{references}{}
\bibliographystyle{plain}
\end{document}
